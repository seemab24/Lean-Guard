# ğŸš¨ Lean Guard â€“ AI-Based Image Moderator

**Lean Guard** is a powerful AI-driven image moderation system designed to automatically classify uploaded images as **Safe** or **Prohibited**. Leveraging deep learning models, it detects sensitive content such as weapons, gore, and explosives â€” helping platforms maintain safe and compliant environments.

---
## âš™ï¸ Tech Stack

- **Frontend**: HTML5, CSS3 (via Flask templating â€“ Jinja2)
- **Backend**: Python 3.10, Flask (micro web framework)
- **Machine Learning**: Keras, TensorFlow, EfficientNet, Custom CNN
- **Deployment**: Localhost (Flask server)
- **Environment**: `venv` (Python virtual environment)
- **Other Tools**: Git, GitHub, Google Colab (for model training and prototyping)

## ğŸ” Features

- ğŸŒ **Interactive Web Interface** for uploading and analyzing images in real-time  
- ğŸ§  **Deep Learning Models** trained on sensitive content categories  
- âš¡ **Instant Feedback** on whether an image is safe or violates content policy  
- ğŸ”’ **Extendable Architecture** for future authentication, analytics, or cloud integrations  

---

## ğŸ§  Models Used

The following Keras-based models are integrated into the backend for prediction:

- `best_weapon_model.keras` â€“ Detects the presence of weapons  
- `best_gore_model.keras` â€“ Identifies gory or violent imagery  
- `best_explosive_model.keras` â€“ Recognizes explosives  
- `custom_cnn_model.keras` â€“ Custom-built CNN for benchmarking  
- `efficientnet_model.keras` â€“ Transfer learning using EfficientNet for high accuracy  

---

## ğŸ“ Folder Structure

```text
LEAN_GUARD/
â”‚
â”œâ”€â”€ images_for_testing/       # Sample images for testing
â”‚   â”œâ”€â”€ safe_samples/         # Approved content examples
â”‚   â””â”€â”€ prohibited_samples/   # Restricted content examples
â”‚
â”œâ”€â”€ models/                   # Pretrained Keras models
â”‚   â”œâ”€â”€ best_explosive_model.keras
â”‚   â”œâ”€â”€ best_gore_model.keras
â”‚   â”œâ”€â”€ best_weapon_model.keras
â”‚   â”œâ”€â”€ custom_cnn_model.keras
â”‚   â””â”€â”€ efficientnet_model.keras
â”‚
â”œâ”€â”€ screenshots/              # Application screenshots
â”‚   â”œâ”€â”€ home.jpg
â”‚   â”œâ”€â”€ safe_image_result.jpg
â”‚   â”œâ”€â”€ safe_image_result_2.jpg
â”‚   â”œâ”€â”€ prohibited_result.jpg
â”‚   â””â”€â”€ prohibited_result_2.jpg
â”‚
â”œâ”€â”€ templates/                # Flask templates
â”‚   â””â”€â”€ index.html            # Main interface
â”‚
â”œâ”€â”€ venv/                     # Python virtual environment
â”‚
â”œâ”€â”€ app.py                    # Flask application
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # Project documentation
```
---


## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

git clone https://github.com/yourusername/lean_guard.git
cd LEAN_GUARD

 
### 2ï¸âƒ£ Create a Virtual Environment


python -m venv venv

### Activate the environment:

####  On Windows:


venv\Scripts\activate


#### On Mac/Linux:


source venv/bin/activate


### 3ï¸âƒ£ Install Dependencies


pip install -r requirements.txt


### 4ï¸âƒ£ Run the Flask App


python app.py
ğŸ”— The app will be accessible at: http://127.0.0.1:8000

## ğŸ’¡ How It Works
User uploads an image via the web interface (index.html)

The Flask backend receives and preprocesses the image

The image is passed through the following models:

- ğŸ”« Weapon detection

- ğŸ©¸ Gore detection

- ğŸ’£ Explosives detection

If any model flags prohibited content, the image is marked Prohibited

The result is returned to the frontend with dynamic feedback and UI updates

## ğŸ§ª Testing
Use the images for testing/ folder to simulate safe and unsafe inputs. These are example images demonstrating the systemâ€™s ability to detect inappropriate content across different classes.

## ğŸ›  Custom Training (Optional)
To build and use your own models:

Prepare labeled datasets for:

Safe vs Weapon

Safe vs Gore

Safe vs Explosives

Train using Keras (CNN or EfficientNet recommended)

Save models as .keras and place them in the models/ folder




## ğŸ–¼ï¸ Screenshots

### ğŸ”¹ Home Page
<img src="Screenshots/home.jpg" alt="Home Page" width="600" style="display: block; margin: 0 auto;"/>
<br>

### ğŸ”¹ Safe Image Results
<img src="Screenshots/safe_image_result.jpg" alt="Safe Result 1" width="600" style="display: block; margin: 0 auto;"/>
<br>

<img src="Screenshots/safe_image_result_2.jpg" alt="Safe Result 2" width="600" style="display: block; margin: 0 auto;"/>
<br>

<img src="Screenshots/safe_image_result_3.jpg" alt="Safe Result 3" width="600" style="display: block; margin: 0 auto;"/>
<br>

<img src="Screenshots/safe_image_result_4.jpg" alt="Safe Result 4" width="600" style="display: block; margin: 0 auto;"/>
<br>

<img src="Screenshots/safe_image_result_5.jpg" alt="Safe Result 5" width="600" style="display: block; margin: 0 auto;"/>
<br>

### ğŸ”¹ Prohibited Image Results
<img src="Screenshots/prohibited_result.jpg" alt="Prohibited Result 1" width="600" style="display: block; margin: 0 auto;"/>
<br>

<img src="Screenshots/prohibited_result_2.jpg" alt="Prohibited Result 2" width="600" style="display: block; margin: 0 auto;"/>
<br>

<img src="Screenshots/prohibited_result_3.jpg" alt="Prohibited Result 3" width="600" style="display: block; margin: 0 auto;"/>
<br>

<img src="Screenshots/prohibited_result_4.jpg" alt="Prohibited Result 4" width="600" style="display: block; margin: 0 auto;"/>


## ğŸ‘¥ Contributors

- [Seemab Hassan](https://www.linkedin.com/in/seemab-hassan-31aaa030a)





## ğŸ“„ License
This project is licensed under the MIT License â€“ feel free to use and modify with credit.

## ğŸ“Œ Future Improvements
User authentication for moderators

Logging and dashboard analytics

Multi-language support

Integration with content management systems or cloud storage
